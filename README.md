
# 中文长文本检索模型微调

如今的大模型就像一位饱读诗书的通才，经过海量通用数据训练后，能应对翻译、写作、问答等多种任务。但就像一位全科医生面对复杂的心脏手术时，不如心脏专科医生精准一样，大模型在具体场景中总有些 “力不从心”。随着大模型技术的成熟，“通用” 已不再是追求的终点。各行各业都需要 AI 解决实际问题：律师要快速从判例中找依据，工程师要从设备手册里查参数，学生要从教材中摘考点。于是，模型微调成了必然选择，让大模型从 “通才” 变成 “专家”，就像给万能钥匙配一把专属锁芯，开锁又快又准。

目前主流大模型的训练数据里，英文占大部分。中文因为一词多义等原因，模型的语义的理解和英文相差巨大。而中文信息正在爆炸式增长：每天有 500 万篇中文新闻发布，100 万篇学术论文用中文撰写，更有数不清的小说、公众号文章。这些信息需要被精准检索，中文模型必须 “自己懂自己”。把模型比作人的话，普通模型像个 “急性子读者”，读到后半段就忘了前半段，常常漏掉关键信息。比如检索 “某公司 2023 年的研发投入”，它可能只看到第一段的总预算，却忽略了第十章里细分的研发费用明细。长文本检索模型就像一位 “细心的编辑”，能逐字逐句梳理逻辑，把藏在角落里的信息都挖出来。


## 基于中文预训练模型的通用微调流程实践

我们设计了一套**预训练模型→专项微调→能力扩展**的完整流程，核心目标很简单：让不同规模、不同架构的预训练模型，都能 “学会” 精准处理中文长文本检索任务。
https://github.com/LeanderLi1014/retrieval/retrieval_1.jpg

<figure>
  <!-- 长文本检索模型微调流程 -->
  <div style="text-align: center; margin-bottom: 20px;">
    <img src="https://github.com/LeanderLi1014/retrieval/raw/main/retrieval_1.jpg" />
    <div style="margin-top: 8px; font-weight: 500;"></div>
  </div>
目前是使用了三个中文预训练模型进行微调，也可以尝试其他的语言预训练模型进行相应语言的长文本检索模型微调。
  
预训练模型本身是 “通用语言专家”，能理解日常中文，但对“长文本里找特定信息”这件事，就像 “全科医生看专科病”—— 懂基本逻辑，但缺专项经验。
————27M 的 Minimind：轻量、速度快，但语义理解浅；
————108M 的 Minimind：平衡了性能和深度；
————494M 的 qwen2：参数大、语义理解细，但训练成本高。

因为预训练模型也是生成模型，所以我们需要修改模型的输出，检索模型不是生成模型，需要的是把检索到的内容原封不动地输出。

## 模型结构改造：适配检索任务的输入-架构-输出重构

为把通用模型改造成擅长做 “查询 - 文本匹配”，我们对基础模型的输入处理、网络架构、输出形式进行了三重改造，让模型从 “通用语言理解” 转向 “精准检索匹配”。

在**输入层**，让模型 “看懂” 中文文本，保持和预训练模型相同的格式，由 Tokenizer 统一处理，生成包含两个核心张量的字典：

{
    "input_ids": torch.Tensor,  
    "attention_mask": torch.Tensor 
}

由tokenizer生成的包含input_ids和attention_mask的字典。input_ids分词后的token IDs（如 “肺癌早期筛查”→[101, 2345, 3456, ..., 102]）。attention_mask表示为token有效位置，（1 = 有效文本，0 = 填充位），避免模型关注无效填充内容。

在**架构层**，为了让 “查询” 和 “文本” 说同一种语言，我们将原 Minimind 的单编码器结构，重构为 Query 塔 + passage 塔的双塔架构，且两塔共享全部网络参数。
这样做的好处就是让查询塔和文档塔各司其职，Query 塔专注编码用户查询（如 “肺癌基因检测方法”），提取查询的核心语义特征。Passage 塔专注编码候选文本（如医学数据库中的论文、病例），提取文本的语义特征。
双塔模型的设计，在小数据集的情况下可能舍弃检索的精度，但是当数据集非常大的时候，单塔模型的实时计算 query-passage 交互，消耗时间复杂度O(N)随文档量线性增长。双塔模型的优势就能体现出来，检索速度相对更快，非常适合工业级的扩展。所以我们的微调实验都基于双塔模型的设计。
在单塔模型里，query 塔和 passage 塔共用一个编码器，内存占比相对更少，所以我们共享权重虽然是双塔模型，但仅需单塔参数量（如 编码器27M的模型，双塔模型的参数量仍为27M），大幅降低训练与推理成本。

在**输出层**，摒弃原模型适用于文本生成的Token 概率分布输出，重构为文本级稠密向量输出

{
    "dense_vecs": torch.FloatTensor  
}
Query/passage 塔的编码器输出经池化（如平均池化），得到核心特征。
再通过全连接层映射到固定维度（128维），输出最终的dense_vecs。

在模型已经设计好了输入输出和架构后，我们就要考虑怎么让模型学习到区分查询，文档库里需要查询匹配的样本段（正样本）和不匹配的样本段（负样本）


## 损失函数定义

给定批次大小 $N$，每个查询对应 $K$ 个负样本，损失函数定义为：

$$\mathcal{L} = -\frac{1}{N}\sum_{i=1}^N \log P(i^+)$$

其中 $P(i^+)$ 表示查询 $i$ 与正样本的匹配概率：

$$P(i^+) = \frac{\exp\left(\frac{\text{sim}(q_i, p^+)}{\tau}\right)}{
\exp\left(\frac{\text{sim}(q_i, p^+)}{\tau}\right) + 
\sum\limits_{j=1}^{N \times K} \exp\left(\frac{\text{sim}(q_i, p_j)}{\tau}\right)
}$$

## 变量说明

| 符号 | 含义 | 范围/性质 |
|------|------|-----------|
| $N$ | 批次大小 | $N \in \mathbb{Z}^+$ |
| $K$ | 每个查询的负样本数 | $K \in \mathbb{Z}^+$ |
| $q_i$ | 第 $i$ 个查询向量 | $\mathbb{R}^d$ |
| $p^+$ | 正样本向量 | $\mathbb{R}^d$ |
| $p_j$ | 负样本向量 | $\mathbb{R}^d$ |
| $\text{sim}(\cdot,\cdot)$ | 相似度函数（如余弦相似度） | $[-1, 1]$ |
| $\tau$ | 温度参数 | $\tau > 0$ |
| $s_{i,j} = \frac{\text{sim}(q_i, p_j)}{\tau}$ | 归一化相似度 | $\mathbb{R}$ |

## 梯度分析

### 正样本方向梯度

$$\frac{\partial \mathcal{L}}{\partial s^+} = \frac{P(i^+) - 1}{N\tau} < 0$$

**性质说明**：  
梯度始终为负，驱使模型：
1. 增大查询与正样本的相似度
2. 缩短正样本在向量空间的距离

### 负样本方向梯度

$$\frac{\partial \mathcal{L}}{\partial s_{i,j}} = \frac{P(i,j)}{N\tau} > 0$$

**性质说明**：  
梯度始终为正，驱使模型：
1. 减小查询与负样本的相似度
2. 推远负样本在向量空间的距离

## 优化动态

| 变化方向 | 损失函数变化 | 优化效果 |
|----------|--------------|----------|
| $\uparrow \text{sim}(q_i, p^+)$ | $\mathcal{L} \downarrow$ | 正样本更接近查询 |
| $\downarrow \text{sim}(q_i, p_j)$ | $\mathcal{L} \downarrow$ | 负样本远离查询 |
| $\uparrow \text{sim}(q_i, p_j)$ | $\mathcal{L} \uparrow$ | 惩罚错误匹配 |
| $\downarrow \text{sim}(q_i, p^+)$ | $\mathcal{L} \uparrow$ | 惩罚正样本远离 |

## 温度参数 $\tau$ 的作用

$$\tau \downarrow \Rightarrow \text{概率分布更尖锐} \quad \tau \uparrow \Rightarrow \text{概率分布更平滑}$$

1. **较小 $\tau$（<0.1）**：  
   - 强化困难负样本的区分度
   - 适用于高质量数据集
   
2. **较大 $\tau$（>0.5）**：  
   - 降低梯度强度
   - 适用于噪声较多数据集



## 物理意义图解

```mermaid
graph LR
    A[查询向量] --> B(正样本)
    A --> C(负样本1)
    A --> D(负样本2)
    
    subgraph 向量空间
    B -->|梯度：负向| A
    C -->|梯度：正向| A
    D -->|梯度：正向| A
    end
    
    classDef positive fill:#d4f7d4,stroke:#2ecc71
    classDef negative fill:#fadbd8,stroke:#e74c3c
    classDef query fill:#d6eaf8,stroke:#3498db
    
    class A query
    class B positive
    class C,D negative



                
