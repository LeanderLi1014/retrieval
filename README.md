
# 中文长文本检索模型微调
## 背景

如今的大模型就像一位饱读诗书的通才，经过海量通用数据训练后，能应对翻译、写作、问答等多种任务。但就像一位全科医生面对复杂的心脏手术时，不如心脏专科医生精准一样，大模型在具体场景中总有些 “力不从心”。随着大模型技术的成熟，“通用” 已不再是追求的终点。各行各业都需要 AI 解决实际问题：律师要快速从判例中找依据，工程师要从设备手册里查参数，学生要从教材中摘考点。于是，模型微调成了必然选择，让大模型从 “通才” 变成 “专家”，就像给万能钥匙配一把专属锁芯，开锁又快又准。

目前主流大模型的训练数据里，英文占大部分。中文因为一词多义等原因，模型的语义的理解和英文相差巨大。而中文信息正在爆炸式增长：每天有 500 万篇中文新闻发布，100 万篇学术论文用中文撰写，更有数不清的小说、公众号文章。这些信息需要被精准检索，中文模型必须 “自己懂自己”。把模型比作人的话，普通模型像个 “急性子读者”，读到后半段就忘了前半段，常常漏掉关键信息。比如检索 “某公司 2023 年的研发投入”，它可能只看到第一段的总预算，却忽略了第十章里细分的研发费用明细。长文本检索模型就像一位 “细心的编辑”，能逐字逐句梳理逻辑，把藏在角落里的信息都挖出来。


## 基于中文预训练模型的通用微调流程实践

我们设计了一套**预训练模型→检索微调→能力扩展**的完整流程，核心目标很简单：让不同规模、不同架构的预训练模型，都能 “学会” 精准处理中文长文本检索任务。
https://github.com/LeanderLi1014/retrieval/retrieval_1.jpg

<figure>
  <!-- 长文本检索模型微调流程 -->
  <div style="text-align: center; margin-bottom: 20px;">
    <img src="https://github.com/LeanderLi1014/retrieval/raw/main/retrieval_1.jpg" />
    <div style="margin-top: 8px; font-weight: 500;"></div>
  </div>
目前是使用了三个中文预训练模型进行微调，也可以尝试其他的语言预训练模型进行相应语言的长文本检索模型微调。
  
预训练模型本身是 “通用语言专家”，能理解日常中文，但对“长文本里找特定信息”这件事，就像 “全科医生看专科病”—— 懂基本逻辑，但缺专项经验。
————27M 的 Minimind：轻量、速度快，但语义理解浅；
————108M 的 Minimind：平衡了性能和深度；
————494M 的 qwen2：参数大、语义理解细，但训练成本高。

因为预训练模型也是生成模型，所以我们需要修改模型的输出，检索模型不是生成模型，需要的是把检索到的内容原封不动地输出。

## 模型结构改造：适配检索任务的输入-架构-输出重构

为把通用模型改造成擅长做 “查询 - 文本匹配”，我们对基础模型的输入处理、网络架构、输出形式进行了三重改造，让模型从 “通用语言理解” 转向 “精准检索匹配”。

在**输入层**，让模型 “看懂” 中文文本，保持和预训练模型相同的格式，由 Tokenizer 统一处理，生成包含两个核心张量的字典：

{
    "input_ids": torch.Tensor,  
    "attention_mask": torch.Tensor 
}

由tokenizer生成的包含input_ids和attention_mask的字典。input_ids分词后的token IDs（如 “肺癌早期筛查”→[101, 2345, 3456, ..., 102]）。attention_mask表示为token有效位置，（1 = 有效文本，0 = 填充位），避免模型关注无效填充内容。

在**架构层**，为了让 “查询” 和 “文本” 说同一种语言，我们将原 Minimind 的单编码器结构，重构为 Query 塔 + passage 塔的双塔架构，且两塔共享全部网络参数。
这样做的好处就是让查询塔和文档塔各司其职，Query 塔专注编码用户查询（如 “肺癌基因检测方法”），提取查询的核心语义特征。Passage 塔专注编码候选文本（如医学数据库中的论文、病例），提取文本的语义特征。
双塔模型的设计，在小数据集的情况下可能舍弃检索的精度，但是当数据集非常大的时候，单塔模型的实时计算 query-passage 交互，消耗时间复杂度O(N)随文档量线性增长。双塔模型的优势就能体现出来，检索速度相对更快，非常适合工业级的扩展。所以我们的微调实验都基于双塔模型的设计。
在单塔模型里，query 塔和 passage 塔共用一个编码器，内存占比相对更少，所以我们共享权重虽然是双塔模型，但仅需单塔参数量（如 编码器27M的模型，双塔模型的参数量仍为27M），大幅降低训练与推理成本。

在**输出层**，摒弃原模型适用于文本生成的Token 概率分布输出，重构为文本级稠密向量输出

{
    "dense_vecs": torch.FloatTensor  
}
Query/passage 塔的编码器输出经池化（如平均池化），得到核心特征。
再通过全连接层映射到固定维度（128维），输出最终的dense_vecs。

在模型已经设计好了输入输出和架构后，我们就要考虑怎么让模型学习到区分查询，文档库里需要查询匹配的样本段（正样本）和不匹配的样本段（负样本）


## 损失函数定义

给定批次大小 $N$，每个查询对应 $K$ 个负样本，损失函数定义为：

$$\mathcal{L} = -\frac{1}{N}\sum_{i=1}^N \log P(i^+)$$

其中 $P(i^+)$ 表示查询 $i$ 与正样本的匹配概率：

$$P(i^+) = \frac{\exp\left(\frac{\text{sim}(q_i, p^+)}{\tau}\right)}{
\exp\left(\frac{\text{sim}(q_i, p^+)}{\tau}\right) + 
\sum\limits_{j=1}^{N \times K} \exp\left(\frac{\text{sim}(q_i, p_j)}{\tau}\right)
}$$

## 变量说明

| 符号 | 含义 | 范围/性质 |
|------|------|-----------|
| $N$ | 批次大小 | $N \in \mathbb{Z}^+$ |
| $K$ | 每个查询的负样本数 | $K \in \mathbb{Z}^+$ |
| $q_i$ | 第 $i$ 个查询向量 | $\mathbb{R}^d$ |
| $p^+$ | 正样本向量 | $\mathbb{R}^d$ |
| $p_j$ | 负样本向量 | $\mathbb{R}^d$ |
| $\text{sim}(\cdot,\cdot)$ | 相似度函数（如余弦相似度） | $[-1, 1]$ |
| $\tau$ | 温度参数 | $\tau > 0$ |
| $s_{i,j} = \frac{\text{sim}(q_i, p_j)}{\tau}$ | 归一化相似度 | $\mathbb{R}$ |

## 梯度分析

### 正样本方向梯度

$$\frac{\partial \mathcal{L}}{\partial s^+} = \frac{P(i^+) - 1}{N\tau} < 0$$

**性质说明**：  
梯度始终为负，驱使模型：
1. 增大查询与正样本的相似度
2. 缩短正样本在向量空间的距离

### 负样本方向梯度

$$\frac{\partial \mathcal{L}}{\partial s_{i,j}} = \frac{P(i,j)}{N\tau} > 0$$

**性质说明**：  
梯度始终为正，驱使模型：
1. 减小查询与负样本的相似度
2. 推远负样本在向量空间的距离

## 优化动态

| 变化方向 | 损失函数变化 | 优化效果 |
|----------|--------------|----------|
| $\uparrow \text{sim}(q_i, p^+)$ | $\mathcal{L} \downarrow$ | 正样本更接近查询 |
| $\downarrow \text{sim}(q_i, p_j)$ | $\mathcal{L} \downarrow$ | 负样本远离查询 |
| $\uparrow \text{sim}(q_i, p_j)$ | $\mathcal{L} \uparrow$ | 惩罚错误匹配 |
| $\downarrow \text{sim}(q_i, p^+)$ | $\mathcal{L} \uparrow$ | 惩罚正样本远离 |

## 温度参数 $\tau$ 的作用

$$\tau \downarrow \Rightarrow \text{概率分布更尖锐} \quad \tau \uparrow \Rightarrow \text{概率分布更平滑}$$

1. **较小 $\tau$（<0.1）**：  
   - 强化困难负样本的区分度
   - 适用于高质量数据集
   
2. **较大 $\tau$（>0.5）**：  
   - 降低梯度强度
   - 适用于噪声较多数据集

最佳的情况，查询与正样本相似度增加，与负样本相似度下降，损失函数下降的方式进行，让模型可以学习相同语义的内容向量的相似。

# 物理意义图解

```mermaid
graph LR
    A[查询向量] --> B(正样本)
    A --> C(负样本1)
    A --> D(负样本2)
    
    subgraph 向量空间
    B -->|梯度：负向| A
    C -->|梯度：正向| A
    D -->|梯度：正向| A
    end
    
    classDef positive fill:#d4f7d4,stroke:#2ecc71
    classDef negative fill:#fadbd8,stroke:#e74c3c
    classDef query fill:#d6eaf8,stroke:#3498db
    
    class A query
    class B positive
    class C,D negative
```

---
上面内容就把检索微调的架构搭建起来了！将中文预训练模型微调为中文长文本检索模型。为突破模型参数量的限制以提升检索能力，需要在预训练模型基础上扩展参数规模。然而，若直接在微调阶段对编码器进行深度和维度的粗暴扩展，会导致两个关键问题：
1. ​​预训练权重加载失效​​：新增的网络层无法匹配预训练模型的参数结构
2. 参数初始化冲突​​：扩展部分被迫使用随机初始化，形成未经过预训练的"空白"参数区
    # 直接扩展的结构性冲突
    原始模型: [PT-L1, PT-L2, PT-L3, PT-L4]  # PT=预训练层
    扩展模型: [PT-L1, PT-L2, PT-L3, PT-L4, RND-L5, RND-L6]  # RND=随机初始化层
这种架构中，随机初始化的高层网络（RND-L5/L6）会破坏底层预训练特征（PT-L1-L4）的完整性，反而导致检索性能退化。为此，我们设计了渐进式解冻机制，其核心思想是：​​让新增层在原始特征稳定后逐步参与训练​​。
具体的扩展机制：
在模型初始化后，新增的层在初始时是冻结的，模型权重是解冻的，新增层随着epoch训练而解冻
### 渐进式解冻策略训练过程

| 训练周期 | 解冻触发 | 训练阶段描述 |
|---------|---------|-------------|
| **Epoch 0** | ❌ | **初始解冻阶段**<br>- 原始层(1-N)和投影层完全解冻<br>- 所有新增层(M+1~M+K)保持冻结<br> |
| **Epoch 1** | ❌ | **稳定强化阶段**<br>- 继续训练原始层和投影层<br>- 新增层保持冻结<br>- 强化预训练特征在检索任务中的适应性 |
| **Epoch 2** | ✅ | **首次扩展阶段**<br>- 解冻第1个新增层(M+1)<br>- 开始训练：原始层(1~N) + 第1新增层 + 投影层<br>|
| **Epoch 3** | ❌ | **协同优化阶段**<br>- 继续训练原始层+第1新增层+投影层<br>- 其他新增层保持冻结<br> |
| **Epoch 4** | ✅ | **二次扩展阶段**<br>- 解冻第2个新增层(M+2)<br>- 开始训练：原始层(1~N) + 第1-2新增层 + 投影层<br> |
| **Epoch 5** | ❌ | **深度协同阶段**<br>- 继续训练原始层+第1-2新增层+投影层<br>- 优化多层特征融合<br>|
| **Epoch 6** | ✅ | **渐进扩展阶段**<br>- 解冻第3个新增层(M+3)<br>- 开始训练：原始层(1~N) + 第1-3新增层 + 投影层<br>|
| **...** | ... | **模式延续**<br>- 每2个周期解冻1个新增层<br> |
| **Epoch N** | - | **完整训练阶段**<br>- 全模型层解冻<br>- 端到端优化所有参数<br> |


## 数据集
项目数据集来源于 bge_m3 模型训练数据集的中文部分（FlagEmbedding 仓库），训练集筛选适合长文本训练的子集包括：
- MLDR 中文部分
- lecardv2_len-7000-inf 部分
- t2ranking
- miracl_zh
- dureader
所有项目共用相同的验证集和测试集
总计约 288K 个样本，统一处理后的单样本结构如下：

```json
{
  "query": "查询文本",
  "positive_passages": [
    {"text": "相关段落1"},
    {"text": "相关段落2"}
  ],
  "negative_passages": [
    {"text": "不相关段落1"},
    {"text": "不相关段落2"}
  ]
}
```

其中：
- `positive_passages` 为与query相关的正样本（包含一个或多个相关段落）
- `negative_passages` 为与query相关的负样本（包含多个不相关段落）


## 实验环境配置
实验环境配置
CPU: Intel(R) Core(TM) i7-10700 @ 2.90GHz
RAM: 128 GB
GPU: NVIDIA GeForce RTX 3060 / Google colab A100
transformers==4.36.0
PyTorch: 2.5.1+cu121
datasets==2.14.0 
Python==3.10.16
## 结果与分析

