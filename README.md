
# 中文长文本检索模型微调

如今的大模型就像一位饱读诗书的通才，经过海量通用数据训练后，能应对翻译、写作、问答等多种任务。但就像一位全科医生面对复杂的心脏手术时，不如心脏专科医生精准一样，大模型在具体场景中总有些 “力不从心”。随着大模型技术的成熟，“通用” 已不再是追求的终点。各行各业都需要 AI 解决实际问题：律师要快速从判例中找依据，工程师要从设备手册里查参数，学生要从教材中摘考点。于是，模型微调成了必然选择，让大模型从 “通才” 变成 “专家”，就像给万能钥匙配一把专属锁芯，开锁又快又准。

目前主流大模型的训练数据里，英文占大部分。中文因为一词多义等原因，模型的语义的理解和英文相差巨大。而中文信息正在爆炸式增长：每天有 500 万篇中文新闻发布，100 万篇学术论文用中文撰写，更有数不清的小说、公众号文章。这些信息需要被精准检索，中文模型必须 “自己懂自己”。把模型比作人的话，普通模型像个 “急性子读者”，读到后半段就忘了前半段，常常漏掉关键信息。比如检索 “某公司 2023 年的研发投入”，它可能只看到第一段的总预算，却忽略了第十章里细分的研发费用明细。长文本检索模型就像一位 “细心的编辑”，能逐字逐句梳理逻辑，把藏在角落里的信息都挖出来。


## 基于中文预训练模型的微调实践

我们设计了一套**预训练模型→专项微调→能力扩展**的完整流程，让通用模型逐步适配中文长文本检索的特殊需求。
https://github.com/LeanderLi1014/retrieval/retrieval_1.jpg

<figure>
  <!-- 长文本检索模型微调流程 -->
  <div style="text-align: center; margin-bottom: 20px;">
    <img src="https://github.com/LeanderLi1014/retrieval/raw/main/retrieval_1.jpg" />
    <div style="margin-top: 8px; font-weight: 500;"></div>
  </div>
目前是使用了三个中文预训练模型进行微调，也可以尝试其他的语言预训练模型进行相应语言的长文本检索模型微调。
  
